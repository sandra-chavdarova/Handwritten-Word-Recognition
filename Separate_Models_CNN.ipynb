{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1UTbU6PxLu5",
        "outputId": "a5e9efa3-344f-4b4c-c2a7-e3395dcf3072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting idx2numpy\n",
            "  Downloading idx2numpy-1.2.3.tar.gz (6.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from idx2numpy) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from idx2numpy) (1.17.0)\n",
            "Building wheels for collected packages: idx2numpy\n",
            "  Building wheel for idx2numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idx2numpy: filename=idx2numpy-1.2.3-py3-none-any.whl size=7903 sha256=30a1626ed4b84e876d84773661579dcba0df584aa4115a7d13af474708f8fbcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/48/00/ae031c97d62f39e1c3c4daa00426c09a65eb29ae5753a189ee\n",
            "Successfully built idx2numpy\n",
            "Installing collected packages: idx2numpy\n",
            "Successfully installed idx2numpy-1.2.3\n",
            "Mounted at /content/drive\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install idx2numpy\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_from_logits(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    correct = (preds == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_acc += accuracy_from_logits(outputs, labels)\n",
        "        total_batches += 1\n",
        "\n",
        "    return running_loss / total_batches, running_acc / total_batches\n",
        "\n",
        "def eval_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_acc += accuracy_from_logits(outputs, labels)\n",
        "            total_batches += 1\n",
        "\n",
        "    return running_loss / total_batches, running_acc / total_batches\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "ZGzoBlf-xtqb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import idx2numpy\n",
        "\n",
        "MNIST_DIR = \"/content/drive/MyDrive/AIProject/numbers\"\n",
        "train_images_path = f\"{MNIST_DIR}/train-images.idx3-ubyte\"\n",
        "train_labels_path = f\"{MNIST_DIR}/train-labels.idx1-ubyte\"\n",
        "test_images_path = f\"{MNIST_DIR}/t10k-images.idx3-ubyte\"\n",
        "test_labels_path = f\"{MNIST_DIR}/t10k-labels.idx1-ubyte\"\n",
        "\n",
        "X_mnist_train = idx2numpy.convert_from_file(train_images_path)\n",
        "y_mnist_train = idx2numpy.convert_from_file(train_labels_path)\n",
        "X_mnist_test = idx2numpy.convert_from_file(test_images_path)\n",
        "y_mnist_test = idx2numpy.convert_from_file(test_labels_path)\n",
        "\n",
        "X_mnist_train = X_mnist_train.astype(np.float32) / 255.0\n",
        "X_mnist_test = X_mnist_test.astype(np.float32)  / 255.0"
      ],
      "metadata": {
        "id": "IX2WkmhGxR6F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add channel dim: (N,28,28) -> (N,1,28,28)\n",
        "if X_mnist_train.ndim == 3:\n",
        "    X_mnist_train = X_mnist_train[:, None, :, :]\n",
        "if X_mnist_test.ndim == 3:\n",
        "    X_mnist_test = X_mnist_test[:, None, :, :]\n",
        "\n",
        "print(\"MNIST shapes:\", X_mnist_train.shape, X_mnist_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a01r5kSsxbs8",
        "outputId": "b778834d-2223-459c-849d-75c53c8c7926"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST shapes: (60000, 1, 28, 28) (10000, 1, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AZ_DIR = \"/content/drive/MyDrive/AIProject/letters kaggle\"\n",
        "AZ_CSV = f\"{AZ_DIR}/A_Z Handwritten Data.csv\"\n",
        "az_df = pd.read_csv(AZ_CSV, header=None)\n",
        "\n",
        "y_az = az_df.iloc[:, 0].values.astype(np.int64)       # 0–25 for A–Z\n",
        "X_az = az_df.iloc[:, 1:].values.astype(np.float32)    # pixels\n",
        "\n",
        "X_az /= 255.0\n",
        "X_az = X_az.reshape(-1, 1, 28, 28)\n",
        "\n",
        "print(\"A_Z shapes:\", X_az.shape, y_az.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxf5e4HExndI",
        "outputId": "853f7a81-f74b-413e-852a-eec7c021bdb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A_Z shapes: (372451, 1, 28, 28) (372451,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_digits = np.concatenate([X_mnist_train, X_mnist_test], axis=0)\n",
        "y_digits = np.concatenate([y_mnist_train, y_mnist_test], axis=0)  # already 0–9\n",
        "\n",
        "X_digits_tensor = torch.from_numpy(X_digits)\n",
        "y_digits_tensor = torch.from_numpy(y_digits)\n",
        "\n",
        "digit_dataset = TensorDataset(X_digits_tensor, y_digits_tensor)\n",
        "\n",
        "total_len = len(digit_dataset)\n",
        "train_len = int(0.8 * total_len)\n",
        "val_len = int(0.1 * total_len)\n",
        "test_len = total_len - train_len - val_len\n",
        "\n",
        "digit_train_ds, digit_val_ds, digit_test_ds = random_split(\n",
        "    digit_dataset, [train_len, val_len, test_len],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "digit_train_dl = DataLoader(digit_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "digit_val_dl = DataLoader(digit_val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "digit_test_dl = DataLoader(digit_test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "digit_model = SimpleCNN(num_classes=10).to(device)\n",
        "digit_criterion = nn.CrossEntropyLoss()\n",
        "digit_optimizer = torch.optim.Adam(digit_model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 15\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = train_one_epoch(digit_model, digit_train_dl, digit_optimizer, digit_criterion, device)\n",
        "    val_loss, val_acc = eval_model(digit_model, digit_val_dl, digit_criterion, device)\n",
        "    print(\n",
        "        f\"DIGITS CNN -> Epoch {epoch}: \"\n",
        "        f\"train loss = {train_loss:.4f}, train accuracy = {train_acc*100:.2f}% | \"\n",
        "        f\"validation loss = {val_loss:.4f}, validation accuracy = {val_acc*100:.2f}%\"\n",
        "    )\n",
        "\n",
        "test_loss, test_acc = eval_model(digit_model, digit_test_dl, digit_criterion, device)\n",
        "print(f\"DIGITS CNN -> Test loss = {test_loss:.4f}, test acc={test_acc*100:.2f}%\")\n",
        "\n",
        "DIGIT_MODEL_PATH = \"/content/drive/MyDrive/AIProject/digit_cnn_10cls.pth\"\n",
        "torch.save(digit_model.state_dict(), DIGIT_MODEL_PATH)\n",
        "print(\"Saved digit model to:\", DIGIT_MODEL_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5EXFNrrxwgk",
        "outputId": "7c782dae-c31f-496e-80d9-8e66d044b862"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIGITS CNN -> Epoch 1: train loss = 0.1819, train accuracy = 94.28% | validation loss = 0.0654, validation accuracy = 98.03%\n",
            "DIGITS CNN -> Epoch 2: train loss = 0.0433, train accuracy = 98.69% | validation loss = 0.0441, validation accuracy = 98.67%\n",
            "DIGITS CNN -> Epoch 3: train loss = 0.0322, train accuracy = 99.02% | validation loss = 0.0359, validation accuracy = 98.87%\n",
            "DIGITS CNN -> Epoch 4: train loss = 0.0240, train accuracy = 99.21% | validation loss = 0.0381, validation accuracy = 98.86%\n",
            "DIGITS CNN -> Epoch 5: train loss = 0.0195, train accuracy = 99.38% | validation loss = 0.0366, validation accuracy = 99.00%\n",
            "DIGITS CNN -> Epoch 6: train loss = 0.0143, train accuracy = 99.55% | validation loss = 0.0326, validation accuracy = 99.02%\n",
            "DIGITS CNN -> Epoch 7: train loss = 0.0122, train accuracy = 99.60% | validation loss = 0.0347, validation accuracy = 98.99%\n",
            "DIGITS CNN -> Epoch 8: train loss = 0.0108, train accuracy = 99.65% | validation loss = 0.0288, validation accuracy = 99.23%\n",
            "DIGITS CNN -> Epoch 9: train loss = 0.0107, train accuracy = 99.64% | validation loss = 0.0399, validation accuracy = 98.91%\n",
            "DIGITS CNN -> Epoch 10: train loss = 0.0074, train accuracy = 99.75% | validation loss = 0.0391, validation accuracy = 99.06%\n",
            "DIGITS CNN -> Epoch 11: train loss = 0.0064, train accuracy = 99.81% | validation loss = 0.0426, validation accuracy = 99.07%\n",
            "DIGITS CNN -> Epoch 12: train loss = 0.0083, train accuracy = 99.72% | validation loss = 0.0364, validation accuracy = 99.11%\n",
            "DIGITS CNN -> Epoch 13: train loss = 0.0050, train accuracy = 99.82% | validation loss = 0.0414, validation accuracy = 99.04%\n",
            "DIGITS CNN -> Epoch 14: train loss = 0.0053, train accuracy = 99.83% | validation loss = 0.0380, validation accuracy = 99.10%\n",
            "DIGITS CNN -> Epoch 15: train loss = 0.0053, train accuracy = 99.82% | validation loss = 0.0514, validation accuracy = 98.82%\n",
            "DIGITS CNN -> Test loss = 0.0509, test acc=98.87%\n",
            "Saved digit model to: /content/drive/MyDrive/AIProject/digit_cnn_10cls.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_letters_tensor = torch.from_numpy(X_az)\n",
        "y_letters_tensor = torch.from_numpy(y_az)\n",
        "\n",
        "letter_dataset = TensorDataset(X_letters_tensor, y_letters_tensor)\n",
        "\n",
        "total_len = len(letter_dataset)\n",
        "train_len = int(0.8 * total_len)\n",
        "val_len = int(0.1 * total_len)\n",
        "test_len = total_len - train_len - val_len\n",
        "\n",
        "letter_train_ds, letter_val_ds, letter_test_ds = random_split(\n",
        "    letter_dataset, [train_len, val_len, test_len],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "letter_train_dl = DataLoader(letter_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "letter_val_dl = DataLoader(letter_val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "letter_test_dl = DataLoader(letter_test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "letter_model = SimpleCNN(num_classes=26).to(device)\n",
        "letter_criterion = nn.CrossEntropyLoss()\n",
        "letter_optimizer = torch.optim.Adam(letter_model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 15\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = train_one_epoch(letter_model, letter_train_dl, letter_optimizer, letter_criterion, device)\n",
        "    val_loss, val_acc = eval_model(letter_model, letter_val_dl, letter_criterion, device)\n",
        "    print(\n",
        "        f\"LETTERS CNN -> Epoch {epoch}: \"\n",
        "        f\"train loss = {train_loss:.4f}, train accuracy = {train_acc*100:.2f}% | \"\n",
        "        f\"validation loss = {val_loss:.4f}, validation accuracy={val_acc*100:.2f}%\"\n",
        "    )\n",
        "\n",
        "test_loss, test_acc = eval_model(letter_model, letter_test_dl, letter_criterion, device)\n",
        "print(f\"LETTERS CNN -> Test loss = {test_loss:.4f}, test accuracy = {test_acc*100:.2f}%\")\n",
        "\n",
        "LETTER_MODEL_PATH = \"/content/drive/MyDrive/AIProject/letter_cnn_26cls.pth\"\n",
        "torch.save(letter_model.state_dict(), LETTER_MODEL_PATH)\n",
        "print(\"Saved letter model to:\", LETTER_MODEL_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWHLOFGsx1Ov",
        "outputId": "6c713c91-0acc-4bff-b3d4-5a115c12e186"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LETTERS CNN -> Epoch 1: train loss = 0.1423, train accuracy = 95.97% | validation loss = 0.0539, validation accuracy=98.55%\n",
            "LETTERS CNN -> Epoch 2: train loss = 0.0477, train accuracy = 98.62% | validation loss = 0.0413, validation accuracy=98.83%\n",
            "LETTERS CNN -> Epoch 3: train loss = 0.0327, train accuracy = 99.02% | validation loss = 0.0335, validation accuracy=99.10%\n",
            "LETTERS CNN -> Epoch 4: train loss = 0.0231, train accuracy = 99.27% | validation loss = 0.0363, validation accuracy=99.05%\n",
            "LETTERS CNN -> Epoch 5: train loss = 0.0169, train accuracy = 99.45% | validation loss = 0.0284, validation accuracy=99.26%\n",
            "LETTERS CNN -> Epoch 6: train loss = 0.0137, train accuracy = 99.55% | validation loss = 0.0252, validation accuracy=99.42%\n",
            "LETTERS CNN -> Epoch 7: train loss = 0.0109, train accuracy = 99.65% | validation loss = 0.0264, validation accuracy=99.41%\n",
            "LETTERS CNN -> Epoch 8: train loss = 0.0094, train accuracy = 99.70% | validation loss = 0.0306, validation accuracy=99.32%\n",
            "LETTERS CNN -> Epoch 9: train loss = 0.0079, train accuracy = 99.74% | validation loss = 0.0242, validation accuracy=99.47%\n",
            "LETTERS CNN -> Epoch 10: train loss = 0.0071, train accuracy = 99.77% | validation loss = 0.0382, validation accuracy=99.27%\n",
            "LETTERS CNN -> Epoch 11: train loss = 0.0067, train accuracy = 99.77% | validation loss = 0.0303, validation accuracy=99.42%\n",
            "LETTERS CNN -> Epoch 12: train loss = 0.0062, train accuracy = 99.80% | validation loss = 0.0417, validation accuracy=99.32%\n",
            "LETTERS CNN -> Epoch 13: train loss = 0.0062, train accuracy = 99.81% | validation loss = 0.0293, validation accuracy=99.56%\n",
            "LETTERS CNN -> Epoch 14: train loss = 0.0050, train accuracy = 99.85% | validation loss = 0.0312, validation accuracy=99.53%\n",
            "LETTERS CNN -> Epoch 15: train loss = 0.0053, train accuracy = 99.84% | validation loss = 0.0303, validation accuracy=99.54%\n",
            "LETTERS CNN -> Test loss = 0.0403, test accuracy = 99.42%\n",
            "Saved letter model to: /content/drive/MyDrive/AIProject/letter_cnn_26cls.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xj2rMT5OySZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}